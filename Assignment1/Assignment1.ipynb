{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to generate random samples following Normal distribution in d dimensions\n",
    "def multi_gaussian(dim, num_of_samples):\n",
    "    cov_matrix = dt.make_spd_matrix(dim) #generate random symmetric, positive definite matrix\n",
    "    mean = np.random.rand(dim)\n",
    "    x = np.random.multivariate_normal(mean, cov_matrix, (num_of_samples)) #shape of x is num_of_samples x dim\n",
    "    return x\n",
    "# multi_gaussian(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_function(x, mean_vec, cov_matrix, dim, prior_prob):\n",
    "        if dim > 1: # multivariate gaussian\n",
    "            cov_inverse = np.linalg.inv(cov_matrix)\n",
    "            cov_det = np.linalg.det(cov_matrix)\n",
    "            return (\n",
    "                    ((- 1/2) * np.square( mahalanobis_dist( x, mean_vec, cov_inverse ))) \\\n",
    "                    - ((dim / 2) * ( np.log(2 * np.pi)) )\\\n",
    "                    - ( (1/2) * np.log(cov_det) ) \\\n",
    "                    + np.log(prior_prob)\n",
    "            )\n",
    "        else: # univariate gaussian\n",
    "#             print(\"euc_dist\",euclidean_dis(x, mean_vec))\n",
    "#             print(cov_matrix)\n",
    "            return (\n",
    "                    -(1/2) * np.square( euclidean_dis(x, mean_vec)) /  cov_matrix \\\n",
    "                    - (1/2) * (np.log(2 * np.pi * cov_matrix))\\\n",
    "                    + np.log(prior_prob)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dis(x, mean_vector):\n",
    "    cov_inverse = np.identity(x.shape[1])\n",
    "    return mahalanobis_dist(x, mean_vector, cov_inverse) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_dist(x, mean_vector, cov_inverse):\n",
    "#     print(\"shape of x in mahalanobis dist\", x.shape)\n",
    "    diff = ( x - mean_vector )\n",
    "    return np.sqrt( np.dot( np.dot(diff.T, cov_inverse), diff ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.01, -8.12, -3.68,  0.  ],\n",
       "       [-5.43, -3.48, -3.54,  0.  ],\n",
       "       [ 1.08, -5.52,  1.66,  0.  ],\n",
       "       [ 0.86, -3.78, -4.11,  0.  ],\n",
       "       [-2.67,  0.63,  7.39,  0.  ],\n",
       "       [ 4.94,  3.29,  2.08,  0.  ],\n",
       "       [-2.51,  2.09, -2.59,  0.  ],\n",
       "       [-2.25, -2.13, -6.94,  0.  ],\n",
       "       [ 5.56,  2.86, -2.26,  0.  ],\n",
       "       [ 1.03, -3.33,  4.33,  0.  ],\n",
       "       [-0.91, -0.18, -0.05,  1.  ],\n",
       "       [ 1.3 , -2.06, -3.53,  1.  ],\n",
       "       [-7.75, -4.54, -0.95,  1.  ],\n",
       "       [-5.47,  0.5 ,  3.92,  1.  ],\n",
       "       [ 6.14,  5.72, -4.85,  1.  ],\n",
       "       [ 3.6 ,  1.26,  4.36,  1.  ],\n",
       "       [ 5.37, -4.63, -3.65,  1.  ],\n",
       "       [ 7.18,  1.46, -6.66,  1.  ],\n",
       "       [-7.39,  1.17,  6.3 ,  1.  ],\n",
       "       [-7.5 , -6.32, -0.31,  1.  ],\n",
       "       [ 5.35,  2.26,  8.13,  2.  ],\n",
       "       [ 5.12,  3.22, -2.66,  2.  ],\n",
       "       [-1.34, -5.31, -9.87,  2.  ],\n",
       "       [ 4.48,  3.42,  5.19,  2.  ],\n",
       "       [ 7.11,  2.39,  9.21,  2.  ],\n",
       "       [ 7.17,  4.33, -0.98,  2.  ],\n",
       "       [ 5.75,  3.97,  6.65,  2.  ],\n",
       "       [ 0.77,  0.27,  2.41,  2.  ],\n",
       "       [ 0.9 , -0.43, -8.71,  2.  ],\n",
       "       [ 3.52, -0.36,  6.43,  2.  ]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('data_dhs_chap2.csv', delimiter=',', skip_header=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomizer(X, Y, dim):\n",
    "    if dim == 1 : # only 1 independent feature\n",
    "        class1_mean_vec = np.mean( X[:5] )\n",
    "        class2_mean_vec = np.mean( X[5:10] )\n",
    "        class1_cov = np.cov( X[:5] )\n",
    "        class2_cov = np.cov( X[5:10] )\n",
    "        shape = (1,1)\n",
    "    else: # more than 1\n",
    "        class1_mean_vec = np.mean( X[:5], axis=0 )\n",
    "        class2_mean_vec = np.mean( X[5:10], axis=0 )\n",
    "        class1_cov = np.cov( X[:5].T )\n",
    "        class2_cov = np.cov( X[5:10].T )\n",
    "        shape = (dim,)\n",
    "    class1_prior_prob = 0.5\n",
    "    class2_prior_prob = 0.5\n",
    "    predicted_class = []\n",
    "    for instance in X:\n",
    "        instance = instance.reshape( shape )\n",
    "        g1 = discriminant_function(instance, class1_mean_vec, class1_cov, dim, class1_prior_prob)\n",
    "        g2 = discriminant_function(instance, class2_mean_vec, class2_cov, dim, class2_prior_prob)\n",
    "        if g1 > g2 :\n",
    "            predicted_class.append(0) # class 1\n",
    "        else :\n",
    "            predicted_class.append(1) # class 2\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_training_error(target_class, predicted_class):\n",
    "    total_instances = len(predicted_class)\n",
    "    error = 0 \n",
    "    for instance in range(total_instances) :\n",
    "        error += np.abs( target_class[instance] - predicted_class[instance] )\n",
    "    avg_error_percent = (100 / total_instances) * error\n",
    "    return avg_error_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted class labels:', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "misclassification error: 50 %\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "trg_data =  data[ 5:15 , [0,3] ] \n",
    "X = trg_data[:, 0] \n",
    "Y = trg_data[:, 1].astype(int)\n",
    "# print(trg_data)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "# print(target_class)\n",
    "predicted_class = dichotomizer(X, Y, 1)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" + str(empirical_training_error(target_class, predicted_class)) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted class labels:', [0, 1, 1, 0, 0, 1, 0, 1, 1, 1])\n",
      "misclassification error: 30 %\n"
     ]
    }
   ],
   "source": [
    "trg_data =  data[ 5:15 , [0,1,3] ] \n",
    "X = trg_data[:, :2] \n",
    "Y = trg_data[:, -1].astype(int)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "predicted_class = dichotomizer(X, Y, 2)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" + str(empirical_training_error(target_class, predicted_class)) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted class labels:', [0, 0, 0, 0, 0, 1, 0, 1, 1, 1])\n",
      "misclassification error: 10 %\n"
     ]
    }
   ],
   "source": [
    "trg_data =  data[ 5:15 , [0,1,2,3] ] \n",
    "X = trg_data[:, :3] \n",
    "Y = trg_data[:, 3].astype(int)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "predicted_class = dichotomizer(X, Y, 3)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" + str(empirical_training_error(target_class, predicted_class)) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.275575314961024, 17.90871050977826, 11.219049962629267]\n",
      "[39.122669886373224, 37.01822619351304, 32.03792965159817]\n",
      "[9.17501158099911, 6.6910663906900005, 3.9810165992798727]\n",
      "[12.018736518930258, 10.307336334245278, 1.3902868654848097]\n",
      "[2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "#2(e)\n",
    "test_data = np.array([\n",
    "    [1,2,1],\n",
    "    [5,3,2],\n",
    "    [0,0,0],\n",
    "    [1,0,0],\n",
    "])\n",
    "\n",
    "class1_samples = data[:10,[0,1,2]]\n",
    "class2_samples = data[:20,[0,1,2]]\n",
    "class3_samples = data[:30,[0,1,2]]\n",
    "\n",
    "class1_mean = np.mean(class1_samples, axis=0)\n",
    "class2_mean = np.mean(class2_samples, axis=0)\n",
    "class3_mean = np.mean(class3_samples, axis=0)\n",
    "# print(class1_mean, class2_mean, class3_mean)\n",
    "# print(class1_mean.shape)\n",
    "\n",
    "class1_cov = np.cov(class1_samples.T)\n",
    "class2_cov = np.cov(class2_samples.T)\n",
    "class3_cov = np.cov(class3_samples.T)\n",
    "# print(class1_cov, class2_cov, class3_cov)\n",
    "\n",
    "predicted_class = []\n",
    "for x in test_data:\n",
    "#     print(x.shape)\n",
    "    distances = []\n",
    "    distances.append( mahalanobis_dist(x, class1_mean, class1_cov) )\n",
    "    distances.append( mahalanobis_dist(x, class2_mean, class2_cov) )\n",
    "    distances.append( mahalanobis_dist(x, class3_mean, class3_cov) )\n",
    "    print(distances)\n",
    "    predicted_class.append(distances.index(min(distances)))\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.09118597961416, 5.0042118472888895, 4.82769396465147]\n",
      "[4.485700025890535, 6.529799857793176, 5.14821405557784]\n",
      "[2.3010829299405198, 4.270385758480403, 4.272408284456987]\n",
      "[2.298420276739546, 4.318965947115872, 4.160693425047617]\n",
      "[1, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# 2(f)\n",
    "dim = 3\n",
    "class1_prior_prob = 0.8\n",
    "class2_prior_prob = 0.1\n",
    "class3_prior_prob = 0.1\n",
    "\n",
    "# print(class1_mean)\n",
    "\n",
    "predicted_class = []\n",
    "\n",
    "for x in test_data:\n",
    "#     print(x.shape)\n",
    "#     print(class1_mean.shape)\n",
    "    values = []\n",
    "    values.append( np.abs( discriminant_function( x, class1_mean, class1_cov, dim, class1_prior_prob )))\n",
    "    values.append( np.abs( discriminant_function( x, class2_mean, class2_cov, dim, class2_prior_prob )))\n",
    "    values.append( np.abs( discriminant_function( x, class3_mean, class3_cov, dim, class3_prior_prob )))\n",
    "    predicted_class.append(values.index(max(values)))\n",
    "    print(values)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Function- Case 1- covariance matrices are equal and proportional to Identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "iris_data = np.genfromtxt('iris.csv', delimiter=',')\n",
    "X = iris_data[:,:4]\n",
    "print(X.shape)\n",
    "label_to_nominal = {\n",
    "    'Iris-setosa' : 0,\n",
    "    'Iris-versicolor' : 1,\n",
    "    'Iris-virginica' : 2\n",
    "}\n",
    "class1 = np.full((1,50), label_to_nominal['Iris-setosa'], dtype=int) # \n",
    "class2 = np.full((1,50), label_to_nominal['Iris-versicolor'], dtype=int) \n",
    "class3 = np.full((1,50), label_to_nominal['Iris-virginica'], dtype=int)\n",
    "Y = np.append(class1, class2)\n",
    "Y = np.append(Y, class3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_samples = X[0:50,]\n",
    "class2_samples = X[50:100,]\n",
    "class3_samples = X[100:150,]\n",
    "# print(class1_samples)\n",
    "\n",
    "iris_class1_mean = np.mean(class1_samples, axis=0)\n",
    "iris_class2_mean = np.mean(class2_samples, axis=0)\n",
    "iris_class3_mean = np.mean(class3_samples, axis=0)\n",
    "\n",
    "iris_class1_cov = np.cov(class1_samples.T)\n",
    "iris_class2_cov = np.cov(class2_samples.T)\n",
    "iris_class3_cov = np.cov(class3_samples.T)\n",
    "\n",
    "# print(class1_cov, class2_cov, class3_cov )\n",
    "# class1_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_discriminant(x, mean_vec, cov_mat, prior_prob):\n",
    "    cov_inverse = np.linalg.inv(cov_matrix)\n",
    "    cov_det = np.linalg.det(cov_matrix)\n",
    "    \n",
    "    Wi = (-1 / 2) * (cov_inverse)\n",
    "    wi = np.dot(cov_inverse, mean_vec)\n",
    "    wio = (-1 / 2) * ( np.dot(np.dot(mean_vec.T, cov_inverse), mean_vec) + np.log(cov_det) ) + np.log(prior_prob) \n",
    "    \n",
    "    g = np.dot(np.dot(x.T, Wi), x) + np.dot(wi.T, x) + wio\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_norm(x, mean_vec):\n",
    "    return np.dot((x - mean_vec).T, (x - mean_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case I : all cov matrices are equal and proportional to I(identity matrix)\n",
    "def case1_linear_discriminant(x, mean_vec, var, prior_prob):\n",
    "    return (\n",
    "        -( euclidean_norm(x, mean_vec) / (2 * np.square(var)) ) + np.log(prior_prob)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15201836734693877"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Case1: var = avg variance (of diagonal elements)\n",
    "# print()\n",
    "\n",
    "diag1 = np.mean(iris_class1_cov.diagonal())\n",
    "diag2 = np.mean(iris_class2_cov.diagonal())\n",
    "diag3 = np.mean(iris_class3_cov.diagonal())\n",
    "\n",
    "var = (diag1+diag2+diag3)/3\n",
    "predicted_class = []\n",
    "for x in X:\n",
    "    g1 = case1_linear_discriminant(x, iris_class1_mean, var, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
