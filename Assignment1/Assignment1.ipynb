{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#method to generate random samples following Normal distribution in d dimensions\n",
    "def multi_gaussian(dim, num_of_samples):\n",
    "    cov_matrix = dt.make_spd_matrix(dim) #generate random symmetric, positive definite matrix\n",
    "    mean = np.random.rand(dim)\n",
    "    x = np.random.multivariate_normal(mean, cov_matrix, (num_of_samples)) #shape of x is num_of_samples x dim\n",
    "    return x\n",
    "# multi_gaussian(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminant_function(x, mean_vec, cov_matrix, dim, prior_prob):\n",
    "        if dim > 1: # multivariate gaussian\n",
    "            cov_inverse = np.linalg.inv(cov_matrix)\n",
    "            cov_det = np.linalg.det(cov_matrix)\n",
    "            return (\n",
    "                    ((- 1/2) * (mahalanobis_dist(x, mean_vec, cov_inverse))**2) \\\n",
    "                    - ((dim / 2) * ( np.log(2 * np.pi)) )\\\n",
    "                    - ( (1/2) * np.log(cov_det) ) \\\n",
    "                    + np.log(prior_prob)\n",
    "            )\n",
    "        else: # univariate gaussian\n",
    "#             print(\"euc_dist\",euclidean_dis(x, mean_vec))\n",
    "#             print(cov_matrix)\n",
    "            return (\n",
    "                    -(1/2) * euclidean_dis(x, mean_vec)**2 /  cov_matrix \\\n",
    "                    - (1/2) * (np.log(2 * np.pi * cov_matrix))\\\n",
    "                    + np.log(prior_prob)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate Euclidean distance between 2 points x and y\n",
    "def euclidean_dis(x, mean_vector):\n",
    "    cov_inverse = np.identity(x.shape[1])\n",
    "    return mahalanobis_dist(x, mean_vector, cov_inverse) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mahalanobis_dist(x, mean_vector, cov_inverse):\n",
    "#     print(\"shape of x in mahalanobis dist\", x.shape)\n",
    "    diff = ( x - mean_vector )\n",
    "    return ( np.dot( np.dot(diff, cov_inverse), diff.T ) ) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('data_dhs_chap2.csv', delimiter=',', skip_header=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dichotomizer(X, Y, dim):\n",
    "    if dim == 1 : # only 1 independent feature\n",
    "        class1_mean_vec = np.mean( X[:5] )\n",
    "        class2_mean_vec = np.mean( X[5:10] )\n",
    "        class1_cov = np.cov( X[:5] )\n",
    "        class2_cov = np.cov( X[5:10] )\n",
    "        shape = (1,1)\n",
    "    else: # more than 1\n",
    "        class1_mean_vec = np.mean( X[:5], axis=0 )\n",
    "        class2_mean_vec = np.mean( X[5:10], axis=0 )\n",
    "        class1_cov = np.cov( X[:5].T )\n",
    "        class2_cov = np.cov( X[5:10].T )\n",
    "        shape = (dim,)\n",
    "    class1_prior_prob = 0.5\n",
    "    class2_prior_prob = 0.5\n",
    "    predicted_class = []\n",
    "    for instance in X:\n",
    "        instance = instance.reshape( shape )\n",
    "        g1 = discriminant_function(instance, class1_mean_vec, class1_cov, dim, class1_prior_prob)\n",
    "        g2 = discriminant_function(instance, class2_mean_vec, class2_cov, dim, class2_prior_prob)\n",
    "        if g1 > g2 :\n",
    "            predicted_class.append(0) # class 1\n",
    "        else :\n",
    "            predicted_class.append(1) # class 2\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def empirical_training_error(target_class, predicted_class):\n",
    "    total_instances = len(predicted_class)\n",
    "    error = 0 \n",
    "    for instance in range(total_instances) :\n",
    "        error += np.abs( target_class[instance] - predicted_class[instance] )\n",
    "    avg_error_percent = (100 / total_instances) * error\n",
    "    return avg_error_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels: [0, 1, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "misclassification error:  50.0\n"
     ]
    }
   ],
   "source": [
    "trg_data =  data[ 5:15 , [0,3] ] \n",
    "X = trg_data[:, 0] \n",
    "Y = trg_data[:, 1].astype(int)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "predicted_class = dichotomizer(X, Y, 1)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" , empirical_training_error(target_class, predicted_class) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels: [0, 1, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "misclassification error:  40.0\n"
     ]
    }
   ],
   "source": [
    "trg_data =  data[ 5:15 , [0,1,3] ] \n",
    "X = trg_data[:, :2] \n",
    "Y = trg_data[:, -1].astype(int)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "predicted_class = dichotomizer(X, Y, 2)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" , empirical_training_error(target_class, predicted_class) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "misclassification error:  0.0\n"
     ]
    }
   ],
   "source": [
    "trg_data =  data[ 5:15 , [0,1,2,3] ] \n",
    "X = trg_data[:, :3] \n",
    "Y = trg_data[:, 3].astype(int)\n",
    "target_class = []\n",
    "for i in Y:\n",
    "    target_class.append(i.item())\n",
    "predicted_class = dichotomizer(X, Y, 3)\n",
    "print(\"predicted class labels:\", predicted_class)\n",
    "print(\"misclassification error: \" , empirical_training_error(target_class, predicted_class) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14.38051111   7.69537778   4.12232222]\n",
      " [  7.69537778  14.62312111   3.90684   ]\n",
      " [  4.12232222   3.90684     19.72453778]]\n",
      "[[ 24.26008711   8.34623342  -5.80606421]\n",
      " [  8.34623342  13.42083658   2.10255053]\n",
      " [ -5.80606421   2.10255053  18.08224632]]\n",
      "[[ 22.87101609  10.42504368   2.52351264]\n",
      " [ 10.42504368  13.0415592    6.33164713]\n",
      " [  2.52351264   6.33164713  27.66965471]]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([\n",
    "    [1,2,1],\n",
    "    [5,3,2],\n",
    "    [0,0,0],\n",
    "    [1,0,0],\n",
    "])\n",
    "\n",
    "class1_samples = data[:10,[0,1,2]]\n",
    "class2_samples = data[:20,[0,1,2]]\n",
    "class3_samples = data[:30,[0,1,2]]\n",
    "\n",
    "class1_mean = np.mean(class1_samples, axis=0)\n",
    "class2_mean = np.mean(class2_samples, axis=0)\n",
    "class3_mean = np.mean(class3_samples, axis=0)\n",
    "\n",
    "class1_cov = np.cov(class1_samples.T)\n",
    "class2_cov = np.cov(class2_samples.T)\n",
    "class3_cov = np.cov(class3_samples.T)\n",
    "\n",
    "predicted_class = []\n",
    "for x in test_data:\n",
    "    distances = []\n",
    "    distances.append( mahalanobis_dist(x, class1_mean, class1_cov) )\n",
    "    distances.append( mahalanobis_dist(x, class2_mean, class2_cov) )\n",
    "    distances.append( mahalanobis_dist(x, class3_mean, class3_cov) )\n",
    "#     print(distances)\n",
    "    predicted_class.append(distances.index(min(distances)))\n",
    "\n",
    "# print(predicted_class)\n",
    "print(class1_cov)\n",
    "print(class2_cov)\n",
    "print(class3_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Function- Case 1- covariance matrices are equal and proportional to Identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('iris.csv', delimiter=',')\n",
    "X = data[:,:4]\n",
    "label_to_nominal = {\n",
    "    'Iris-setosa' : 0,\n",
    "    'Iris-versicolor' : 1,\n",
    "    'Iris-virginica' : 2\n",
    "}\n",
    "class1 = np.full((1,50), label_to_nominal['Iris-setosa'], dtype=int) # \n",
    "class2 = np.full((1,50), label_to_nominal['Iris-versicolor'], dtype=int) \n",
    "class3 = np.full((1,50), label_to_nominal['Iris-virginica'], dtype=int)\n",
    "Y = np.append(class1, class2)\n",
    "Y = np.append(Y, class3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_samples = X[0:50,]\n",
    "class2_samples = X[50:100,]\n",
    "class3_samples = X[100:150,]\n",
    "\n",
    "class1_mean = np.mean(class1_samples, axis=0)\n",
    "class2_mean = np.mean(class2_samples, axis=0)\n",
    "class3_mean = np.mean(class3_samples, axis=0)\n",
    "\n",
    "class1_cov = np.cov(class1_samples.T)\n",
    "class2_cov = np.cov(class2_samples.T)\n",
    "class3_cov = np.cov(class3_samples.T)\n",
    "\n",
    "class1_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_discriminant(x, mean_vec, cov_mat, prior_prob):\n",
    "    cov_inverse = np.linalg.inv(cov_matrix)\n",
    "    cov_det = np.linalg.det(cov_matrix)\n",
    "    \n",
    "    Wi = (-1 / 2) * (cov_inverse)\n",
    "    wi = np.dot(cov_inverse, mean_vec)\n",
    "    wio = (-1 / 2) * ( np.dot(np.dot(mean_vec.T, cov_inverse), mean_vec) + np.log(cov_det) ) + np.log(prior_prob) \n",
    "    \n",
    "    g = np.dot(np.dot(x.T, Wi), x) + np.dot(wi.T, x) + wio\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opencv]",
   "language": "python",
   "name": "conda-env-opencv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
